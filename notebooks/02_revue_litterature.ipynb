{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revu de la litterature\n",
    "\n",
    "Dans ce notebook, nous présentons les principales conclusions tirées de nos différentes lectures sur la classification fine d’images. Nous analysons les approches existantes, les défis rencontrés et examinons la possibilité d'en tirer bénéfice  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c51c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Pourquoi le deep-leaning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a71083",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre dataset, la question que nous nous posons en premier est bien comment développer un modèle de classification d’images. Sur ce point, nos recherches nous ont naturellement conduits vers le deep‑learning. Mais déjà, pourquoi le deep‑learning ? Pourquoi ne pas faire du machine learning classique, notre bonne vieille régression linéaire que nous avons tant de peine à entraîner, à ajuster les hyperparamètres, à optimiser ect?\n",
    "\n",
    "Tout simplement parce que les images ne se réduisent pas à un simple tableau de chiffres indépendants : chaque pixel a une relation spatiale avec ses voisins, et les caractéristiques qui président à la reconnaissance d’un objet (bords, textures, motifs…) sont difficiles à extraire manuellement. Avec une régression ou un SVM “classique”, il faut d’abord fabriquer ces features à la main (Histogram of Oriented Gradients, SIFT, LBP, etc.), puis espérer qu’elles suffisent à séparer nos classes. Le deep‑learning, et plus précisément les réseaux de neurones convolutifs , offre une alternative puissante : ces modèles apprennent automatiquement des représentations hiérarchiques, partant de motifs simples comme la détection de bords pour aboutir à des structures complexes (visage, véhicule, animal…).\n",
    "> Oui mais sur http://vision.stanford.edu/aditya86/ImageNetDogs/ i il y a des features, pourquoi ne les avez‑vous pas pris ?\n",
    "\n",
    "Effectivement, des features sont disponibles sur ce site, mais nous avons décidé de ne pas les utiliser, car cela s’apparente à du « cheat code » :\n",
    "\n",
    "- Manque de généralisabilité : ces features ont été extraites spécifiquement pour ce dataset. Si nous obtenions de bonnes performances dessus, nous ne pourrions pas reproduire les mêmes résultats sur un autre jeu de données.\n",
    "\n",
    "- Objectif pédagogique : notre premiére objectif est d'apprendre les ficelles du metiers . Comment construire un pipeline de classification d’images, depuis l’extraction automatique de représentations jusqu’à l’inférence? En travaillant directement sur les pixels, nous nous assurons que chaque étape (prétraitement, augmentation, architecture, entraînement) est maîtrisée et reproductible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f2189",
   "metadata": {},
   "source": [
    "## 2. TensorFlow vs PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a38a0",
   "metadata": {},
   "source": [
    "Quand on s’intéresse au deep learning en Python, deux géants émergent naturellement : PyTorch et TensorFlow.\n",
    " Chacun a ses atouts, et notre cheminement pour faire un choix tient à la fois de la curiosité technique et de nos contraintes matérielles ; laissez-nous vous raconter.\n",
    "Pytorch est plustôt orienté recherche (https://paperswithcode.com/ en est une trés bonne illustration) tandisque Tensorflow est plus focus indistrue-entreprise et donc toute les aspect de deploiement sont bien encré dans sa philosophie . Tous les deux sont pertinent et en toute objectivité les deux se valent .\n",
    "\n",
    "> deep-leaning ok,mais un frein est rapidement apparu :\n",
    "\n",
    "mais un frein est rapidement apparu :\n",
    "Nous avons choisi TensorFlow, pour principalement  deux raisons :\n",
    "- La documentation parfaite de https://www.tensorflow.org/tutorials?hl=fr (ecrit en français avec tuto + notebooks)\n",
    "- Nos ordinateurs personnelles n’étaient tout simplement pas taillées pour entraîner les mastodontes du deep learning.\n",
    "\n",
    "À ce stade, une question s’est posée naturellement : comment faire tourner un entraînement lourd avec les moyens du bord ?\n",
    "Et là, une idée a surgi : Spark.\n",
    "\n",
    "Spark, on en entend parler partout. C’est l’outil star du traitement distribué. Des tonnes de vidéos, de tutoriels, de conférences. On s’est dit : “Si tout le monde l’utilise, c’est sûrement la bonne voie.”\n",
    "\n",
    "On s’est lancés. PySpark installé, un peu de lecture… puis la réalité nous a vite rattrapés.\n",
    "Notre dataset ? Quelques dizaines de milliers d’images.\n",
    "Spark, lui, est conçu pour manipuler des volumes massifs de données, des teraoctets de logs, des flux en continu, des bases gigantesques.\n",
    "\n",
    "> En clair : notre projet ne pesait pas lourd face à l’écosystème Spark.\n",
    " \n",
    "> Le tournant TensorFlow : distribuer sans complexité\n",
    "\n",
    "C’est en revenant à TensorFlow, un peu par hasard, qu’on a découvert ce qui allait réellement nous convenir.\n",
    "En explorant la documentation officielle — claire, bien structurée et disponible en français — on est tombés sur une perle : tf.distribute.Strategy et ses cousins proches.\n",
    "\n",
    "Plus besoin de Spark, TensorFlow prend en charge l’entraînement multi-GPU,multi-CPU et multi-machine nativement, de façon simple et efficace.\n",
    "Et cerise sur le gateau tout reste compatible grace a notre image docker ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a61b3c",
   "metadata": {},
   "source": [
    " # 1. Les reférences\n",
    "Ce projet machin machin macin\n",
    ">Il s'agira alors de machin machin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c004061",
   "metadata": {},
   "source": [
    "# 2. xxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9704a7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
